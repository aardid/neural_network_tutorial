{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a241f62d",
   "metadata": {},
   "source": [
    "# Coding a simple neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba954a",
   "metadata": {},
   "source": [
    "Here we will learn backpropagation through a toy example just using the NumPy library (linear algebra library).\n",
    "We will implement a 2 layer neural network. One layer for the input, one layer for the output.  \n",
    "\n",
    "To do: \n",
    "- Import the numpy library as np. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa55727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04791979",
   "metadata": {},
   "source": [
    "Neural networks are trained to predict an output from an input, so it is basically a function. \n",
    "\n",
    "In this case we will train a NN that received three binary numbers and output 1 binary number, and we will train it using the following data: \n",
    "\n",
    "                                 Input\t     Output\n",
    "                                0, 0, 0\t       0\n",
    "                                1, 1, 1\t       1\n",
    "                                1, 0, 1\t       1\n",
    "                                0, 1, 1\t       0\n",
    "\n",
    "We will predict the second column given the three input columns by coding the backpropagation in its simplest form. Run the next box to check how our network looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the network\n",
    "from IPython.display import Image\n",
    "Image(\"img/nn_2_draw.png\", width = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef2d74",
   "metadata": {},
   "source": [
    "Impressive right? Let's go back to work.\n",
    "\n",
    "To do: \n",
    "- Initialize the input dataset as a numpy matrix. Each row is a single \"training example\". Each column corresponds to one of our input nodes. Thus, we will have 3 input nodes to the network and 4 training examples.\n",
    "- Initialize the output dataset. In this case, generate the dataset horizontally (with a single row and 4 columns) and add a \".T\" to consider its transpose. After the transpose, this y matrix has 4 rows with one column. Just like our input, each row is a training example, and each column (only one) is an output node. So, our network has 3 inputs and 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset matrix where each row is a training example\n",
    "X = np.array([[_,_,_],\n",
    "              [_,_,_],\n",
    "              [_,_,_],\n",
    "              [_,_,_]])\n",
    "\n",
    "# Output dataset matrix where each row is a training example          \n",
    "y = np.array([[_,_,_,_]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fc2f5",
   "metadata": {},
   "source": [
    "We will be using the Sigmoid function and its derivative several times, so let's defined it as a function. \n",
    "A sigmoid function maps any value to a value between 0 and 1. This is what gives us a probability as output. It also has several other desirable properties for training neural networks. \n",
    "A desirable and useful property of a sigmoid function is that its output can be used to create its derivative. The derivative of the sigmoid's function output 'z' is simply 'z*(1-z)' (for details on this derivative, open sig_der.ng in img folder)\n",
    "\n",
    "To do:\n",
    "- Code the 'sigmoid' function with the sigmoid function.\n",
    "- Code the 'dsigmoid' function with the derivative of the sigmoid function.\n",
    "- Test your functions in the testing section (you need to get the 'correct' output for both functions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-_))\n",
    "# derivative of sigmoid function\n",
    "def dsigmoid(x):\n",
    "    return _\n",
    "\n",
    "# testing\n",
    "if [sigmoid(0),np.round(sigmoid(1),1),np.round(sigmoid(-1),1)] == [0.5, 0.7, 0.3]:\n",
    "    print('sigmoid correct')\n",
    "if [dsigmoid(0),np.round(dsigmoid(1),1),np.round(sigmoid(-1),1)] == [0.25, 0.2, 0.3]:\n",
    "    print('dsigmoid correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be1831",
   "metadata": {},
   "source": [
    "We will seed the random numbers to be created. Your numbers will still be randomly distributed, but they'll be randomly distributed in exactly the same way each time you train. This is a good practice as it makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759744f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17be9b2",
   "metadata": {},
   "source": [
    "Next, we need to set the weight matrix for this neural network. We will call it \"w0\" to imply \"weights zero\". Since we only have 2 layers (input and output), we only need one matrix of weights to connect them. Because we have 3 inputs and 1 output, its dimension should be (3,1) (Also, l0 is of size 3 and l1 is of size 1, that's another useful way to think about it). Thus, we want to connect every node in l0 to every node in l1, which requires a matrix of dimensionality (3,1). \n",
    "\n",
    "Also, we will need to initialize it randomly with a mean of zero. There's no agreement on how to initialize weights, but a mean of zero in weight initialization is an acceptable one.\n",
    "\n",
    "One thing to notice here is that the \"neural network\" is really this matrix, as it contains the values from where we will calculate the output from the inputs. All the learnings will be updated here. \n",
    "\n",
    "To do:\n",
    "- Construct the weigth matrix with cero mean (use np.random.random() https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec308db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer of weights, 'w0', connecting layer 0, 'l0', to layer 1, 'l1' (initialize weights randomly with mean 0)\n",
    "w0 = 2*np.random.random((_,_)) - 1\n",
    "\n",
    "# for the sake of this tutorial, we will set these weights to constant values for now:\n",
    "w0 = np.array([[0.06633057],[0.38375423],[-0.36896874]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd81edb",
   "metadata": {},
   "source": [
    "Now we need to begin the network training. We will define a 'loop' that iterates multiple times over the training to optimize the weights of our network. \n",
    "\n",
    "But first, just to develop some intuition, we will calculate the first step of the loop outside. \n",
    "Let's begin by calculating the first layer, which is simply our data. Remember that our input dataset matrix contains 4 training examples (rows). We will be implementing a 'full batch' training, where we're going to process all of them at the same time (we have 4 different l0 rows, but you can think of it as a single training example).\n",
    "\n",
    "To do:\n",
    "- Define the first layer, l0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "l0 = _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5bf09",
   "metadata": {},
   "source": [
    "The second layer is the predictions of our neural network. First, we let the network predict the output given the input with the current weights. Then, we will check how it matches the true output and adjusts it a bit better in every iteration. \n",
    "When defining the second layer, remember that matrix multiplication is ordered, such the dimensions in the middle of the equation must be the same. The final matrix generated is thus the number of rows of the first matrix and the number of columns of the second matrix. \n",
    "\n",
    "Our input matrix contains 4 training examples, and the neural network should output 4 numbers, that means a (4 x 1) matrix. Each output is a network's guess for a given input.\n",
    "\n",
    "To do: \n",
    "- Calculate the second layer, l1, by forward propagation. Uncomment the print commands to check the shape of the variables involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shapes \n",
    "#print(l0.shape)\n",
    "#print(w0.shape)\n",
    "\n",
    "# Forward propagation\n",
    "l1 = sigmoid(np.dot(l0,w0))\n",
    "\n",
    "# check shape \n",
    "#print(l1.shape)\n",
    "\n",
    "# testing\n",
    "if (np.round(l1[0],1) == 0.5):\n",
    "    print('l1 correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9cdae",
   "metadata": {},
   "source": [
    "Now lets calculate the error given tha l1 has our initial guest for the each input. The error is just a vector of positive and negative numbers reflecting how much the network missed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b90ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error calculation\n",
    "l1_error = y - _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcb934",
   "metadata": {},
   "source": [
    "The next step is where things get interesting. Now we need to calculate the 'error weighted derivative' that we are going to use later to update the weights.\n",
    "\n",
    "To do:\n",
    "- Calculate the derivative of the Sigmoid function evaluated in layer 1. Multiply it by the error in layer 1 (elementwise,*) to get the error weighted derivative 'l1_delta'. Note that l1_error is a (4,1) matrix. sigmoid(l1,True) returns a (4,1) matrix, so an element-wise multiplication should return a (4,1) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta calculation: multiply how much we missed by the slope of the sigmoid at the values in l1\n",
    "l1_delta = _ * _\n",
    "\n",
    "# check shape \n",
    "#print(l1_delta.shape)\n",
    "\n",
    "# testing\n",
    "if (np.round(l1_delta[0],1) == -0.1):\n",
    "    print('l1_delta correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758de6a5",
   "metadata": {},
   "source": [
    "There is a lot going on in the last line. Let's divided it into two parts. \n",
    "\n",
    "First: the derivative. Run the next command to plot a sigmoid function. \n",
    "Let's consider that l1 values are the color dots show long the sigmoid, and the lines are their respective derivatives (slopes). The key aspect to notice here is that when x is close to 0 the slope is high (blue dot) and when x gets bigger the slope gets much smaller (green dot). \n",
    "\n",
    "Second: multiplying the derivative by the error in layer 1. By multiplying the slope by the error we are reducing the error of high confidence predictions. For example, if the slope is flat (like in the green dot), it means that the guessed by the network is very high, like x=2, y=0.9 (or very low). This means that the network was quite confident one way or the other. On the other hand, if the network guessed something close to x=0, y=0.5, then it isn't very confident. So, by multiplying the error by the derivative, we relatively amplify the error of not confident estimations compare to the high confident ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/sigmoid.png\", width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a69b30",
   "metadata": {},
   "source": [
    "Now we are ready to update the weights!\n",
    "\n",
    "To do: \n",
    "- Calculate the '_add' term (this is an auxiliar variable that then we will be adding to the weigth to update it) by the dot product between layer 0 and the error weighted derivative. Remember that the dot product of two vectors is between vectors that share columns (first vector) and rows (second vector).  \n",
    "- Update weights by adding to the current value the dot product between layer 0 and the error weighted derivative ('_add') term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4224be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate term to be added to current weigths\n",
    "_add = np.dot(_.T,_)\n",
    "\n",
    "# check shape \n",
    "#print(_add.shape)\n",
    "# check values. Which weight is being incremented more?\n",
    "#print(_add)\n",
    "\n",
    "# testing\n",
    "if (np.round(_add[0],1) == 0.2):\n",
    "    print('_add term correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ac1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights\n",
    "w0 = w0 + _add\n",
    "# note: every time that you execute this comand it will update w0 (run it just once)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6513a",
   "metadata": {},
   "source": [
    "If you got it all correct well done! Now you are ready to code the loop to train your neural network. We will define a 'for' loop that 'iterates' 10.000 times over the training code to optimize our network (weights) to the dataset.\n",
    "\n",
    "To do:\n",
    "- Using the code developed in previous sections, complete the for loop and test it. Remember that in python for loops, indentation matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer of weights, w0, connecting layer 0, l0, to layer 1, l1 (initialize weights randomly with mean 0)\n",
    "w0 = 2*np.random.random((_,_)) - 1\n",
    "\n",
    "for iter in range(_):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = _\n",
    "    l1 = _\n",
    "\n",
    "    # error calculation\n",
    "    l1_error = _\n",
    "\n",
    "    # delta calculation\n",
    "    l1_delta = _\n",
    "\n",
    "    # update weights\n",
    "    w0 += _\n",
    "    \n",
    "    if (False and (iter% 1000) == 0):\n",
    "        print(\"layer 1 values at iteration \"+str(iter)+\" :\"+str(np.mean(np.abs(l1))))\n",
    "    if (False and (iter% 1000) == 0):\n",
    "        print(\"Error at layer 1 at iteration \"+str(iter)+\" :\"+str(np.mean(np.abs(l1_error))))\n",
    "\n",
    "# check results\n",
    "for i, yt in enumerate(y):\n",
    "    if yt != int(np.round(l1[i])):\n",
    "        print('Test not passed')\n",
    "        print('Layer 1 after last iteration:')\n",
    "        print(np.round(l1))\n",
    "        break\n",
    "    else:\n",
    "        if i == len(y)-1:\n",
    "            print('Test passed!')\n",
    "            print('Layer 1 after last iteration:')\n",
    "            print(np.round(l1))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189aa24",
   "metadata": {},
   "source": [
    "If you got the test passed means that your NN works! But what does it mean that it works? Well, basically that we have calibrated the weights based on the data, and now any input with the same structure of the input dataset could be run throw the network and generate an output. The whole idea is that the calibrated weights captured some patterns of the transformation by just seeing the inputs and the outputs.\n",
    "\n",
    "Why is this useful? For example, if we had a very complicated, time-consuming mathematical function to transforms inputs into outputs, we could use our calibrated neural network to give a really quick output. This is what is called a surrogate model. Could you think in any context that this could be applied? \n",
    "\n",
    "Take some time to play around with the code to get more intuition on how it works. For example, you could:\n",
    "- Check how 'l1' changes every 1000 iteration (set to True the first 'if' statement on the loop).  \n",
    "- Check out how 'l1_error' changes as you iterate (set to True the second 'if' statement on the loop).  \n",
    "- Take a close look at the line where 'l1_delta' is defined. Here is where the magic happens.\n",
    "- Check out the line where we defined 'w0'. Everything in the network prepares for this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b923b56",
   "metadata": {},
   "source": [
    "# Acknowledgement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c26663",
   "metadata": {},
   "source": [
    "This toy example has been adapted from the following blog:\n",
    "\n",
    "https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "Many thanks to the author!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
