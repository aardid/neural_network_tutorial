{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ea925a",
   "metadata": {},
   "source": [
    "# Coding a deep neural network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44579f14",
   "metadata": {},
   "source": [
    "Keras is a friendly (and powerful) python library design for developing and evaluating deep learning models (https://keras.io/).\n",
    "\n",
    "In this tutorial, we will construct a deep neural network using Keras, train it with a dataset for a simple classification task, and evaluate its performance.\n",
    "\n",
    "Let's start importing the libraries to be used in this tutorial. Run the next section by clicking on it and press Ctrl+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eac885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78283e8d",
   "metadata": {},
   "source": [
    "# First step: Load and split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142576e",
   "metadata": {},
   "source": [
    "We are going to use a well-known diabetes dataset (a typical machine learning repository). It describes certain health parameters for a population of Pima Indians and whether they had an onset of diabetes within five years. For our purpose, this is a binary classification problem: if the person has diabetes is labeled as 1, and if not as 0. Let's check the dataset:\n",
    "\n",
    "In the home folder of this tutorial, open the file 'pima-indians-diabetes.csv'. It contains 9 columns, where the first 8 are numerical values that we call attributes indicating a subject characteristic like age, blood pressure, body mass index among others. The last column indicates the binary classification. The dataset contains records for 768 subjects, that we call instances. Several constraints were placed on the selection of these instances from a larger database.  In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "Dataset details can be check here: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names. \n",
    "\n",
    "To do:\n",
    "- Load the Pima Indians diabetes dataset using the NumPy method 'loadtxt'.\n",
    "\n",
    "Note: From now on, you will need to fill the _ (underscores) in the codes given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee62945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = np.loadtxt('_', delimiter=',')\n",
    "\n",
    "# Uncomment and rerun for a quick view on pars distribution\n",
    "#from aux_func import plot_hist\n",
    "#plot_hist(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86357ef7",
   "metadata": {},
   "source": [
    "To properly evaluate our model, we need to split the dataset into training and testing subsets. Why? Because we will be testing the performance of our model with data not employed in training. If the model performs similarly in both datasets, this is indicating a robust model. \n",
    "\n",
    "To do:\n",
    "- Split the dataset (by rows), keeping 90% for the training and 10% for the testing.  \n",
    "- Check that you pass the test (output 'Correct split') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some usefull variables for spliting\n",
    "p = _ # % percentage of testing data\n",
    "nr = len(dataset[:,0]) # total number of instances (rows)\n",
    "nt = int(nr*(p/100)) # number of instances for testing\n",
    "# split into input (X) and output (y) variables for training and testing\n",
    "X_train = dataset[:-_,0:8]\n",
    "y_train = dataset[:-_,_]\n",
    "# for testing \n",
    "X_test = dataset[-_:,0:8]\n",
    "y_test = dataset[-_:,_]\n",
    "\n",
    "# checking\n",
    "if nr == (len(y_test)+len(y_train)):\n",
    "    print('Correct split')\n",
    "else:\n",
    "    print('Incorrect split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dc818",
   "metadata": {},
   "source": [
    "# Second step: Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7699c",
   "metadata": {},
   "source": [
    "Now we need to define our neural network model, which is basically a sequence of neuron layers connected by weights. We will use Keras for defining this sequence of layers. \n",
    "We will create a 'Sequential model' (Keras class: https://keras.io/api/models/sequential/) and add layers one at a time to build our network architecture. We will also use the 'Dense' class to define the fully connected neuron layers and specify the activation function between them. \n",
    "\n",
    "In this example, we will use a fully connected neural network structure with four layers. \n",
    "\n",
    "To do:\n",
    "- Create an object called 'model' from the Sequential class (done). For more information on the Keras Sequential class visit https://keras.io/guides/sequential_model/.\n",
    "- Add a the first hidden layer and the input layer. Ensure that the input layer has the right number of input features. This can be specified when creating the first hidden layer with the input_dim argument.\n",
    "- Add a second hidden layer with 12 neurons (nodes). \n",
    "- Add a third hidden layer with 8 neurons (nodes). \n",
    "- Add a forth output layer.\n",
    "- Check that the model is correct by printing a model summary. \n",
    "\n",
    "Note: the shape of the input to the model is defined as an argument on the first hidden layer. This means that the line of code that adds the first Dense layer is doing 2 things, defining the input or visible layer and the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3da232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(_, input_dim=_, activation='relu')) #relu: rectified linear unit activation function\n",
    "model.add(Dense(_, activation='relu'))\n",
    "model.add(Dense(_, activation='sigmoid')) #sigmoid: Sigmoid function\n",
    "\n",
    "# checking\n",
    "print(model.summary())\n",
    "# draw the network (this just work for the architecture 8-12-8-1)\n",
    "from IPython.display import Image\n",
    "Image(\"img/nn_1_draw.png\", width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9850449",
   "metadata": {},
   "source": [
    "How do we know the number of layers and their types?\n",
    "This is a tough question. Often the best network structure is found through a process of trial and error experimentation. Generally, it would help if you had a network large enough to capture the structure of the problem (this will become more clear in the next two tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40076f43",
   "metadata": {},
   "source": [
    "# Third step: Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2bd43",
   "metadata": {},
   "source": [
    "Compile means convert the code into a machine code (or low-level code) from where the code can be executed. When we execute it, Keras use numerical libraries as TensorFlow or Theano to choose the best way to represent the network for training and making predictions to run on your hardware.\n",
    "\n",
    "Also, when compiling we need to specify how the training is going to be conducted. Remember that training means finding the best set of weights to map inputs to outputs in the dataset. In this case, we need to specify the loss function (to evaluate the sets of weights), the optimizer (like gradient descent), and some particular metrics that we would like to collect. \n",
    "\n",
    "To do:\n",
    "- Compile our Keras Sequential model using the 'compile' method. Use as the loss function 'binary_crossentropy', as optimizer 'adam' (an efficient stochastic gradient descent algorithm), and the metric 'accuracy'.\n",
    "\n",
    "For details go to:\n",
    "- binary_crossentropy: https://keras.io/api/losses/\n",
    "- adam: https://keras.io/api/optimizers/adam/\n",
    "- accuracy: https://keras.io/api/metrics/accuracy_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss=_, optimizer=_, metrics=[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461c5b8",
   "metadata": {},
   "source": [
    "# Fourth step: Fit the model to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d3d6d",
   "metadata": {},
   "source": [
    "Now we are ready to execute the model on some data.\n",
    "We can simply train our model calling the 'fit' method in our Keras model. \n",
    "\n",
    "The training occurs over 'epochs' (epoch: one pass through all of the rows in the training dataset), and each epoch is split into 'batches' (batch: one or more samples considered by the model within an epoch before weights are updated). In other words, epochs are the number of iterations that the training process will go through and the batch the number of samples that it will consider in each iteration to update the weights.\n",
    "\n",
    "To do: \n",
    "- Fit the model to the training dataset. Set the epochs to 300 and the batch size to 10. The first two arguments of the fit method are the input and output of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(_, _, epochs=_, batch_size=_, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9f23f",
   "metadata": {},
   "source": [
    "# Fifth step: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87f851",
   "metadata": {},
   "source": [
    "Now that we have trained our network, we are ready to evaluate it. First, let's check how it performs of the training dataset. We will use the 'evaluate' method in our Keras model and report the accuracy. The method will generate a prediction for each input and compare model outputs with the true values for the training dataset and calculate the accuracy as a percentage of correct results (the method collects scores, including the average loss and any metrics you have configured, such as accuracy).\n",
    "\n",
    "To do:\n",
    "- Evaluate the model accuracy on the training dataset. The two arguments of the evaluate method are the input and output of the training dataset. \n",
    "\n",
    "Note: The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset. We are only interested in reporting the accuracy, so we will ignore the loss value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eeb2b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate the keras model on training data\n",
    "loss, accuracy = model.evaluate(_, _)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced1949",
   "metadata": {},
   "source": [
    "Around 80%? that sounds great, right? Yeah, nah. It's easy to perform great on data that the model has already seen. The true test needs to be done on data not seen by the network during training.\n",
    "\n",
    "To do:\n",
    "- Evaluate the model accuracy on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275291f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model 80 test data\n",
    "loss, accuracy = model.evaluate(_, _)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b9eee",
   "metadata": {},
   "source": [
    "Not bad right? What? Were you expecting? a 100% ðŸ¤£? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e06227",
   "metadata": {},
   "source": [
    "# Sixth step: Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4780c130",
   "metadata": {},
   "source": [
    "Now you can use your Keras trained (and tested) model to make predictions given a certain input. \n",
    "\n",
    "For example, we receive the following data from a new person (instance) beloging to the same group (pima indians): \n",
    "\n",
    "5,166,72,19,175,25.8,0.587,51\n",
    "\n",
    "We can now use our neural network to predict if this person has diabetes (or could develop it). We can use the 'predict' method in our model. \n",
    "\n",
    "To do:\n",
    "- Predict if the new person has or could develop diabetes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5304bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input new instance\n",
    "x_new = np.array([[_,_,_,_,_,_,_,_]])\n",
    "# make probability predictions with the model\n",
    "predictions = model.predict(_)\n",
    "print('Output: '+str(np.round(predictions[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dc205",
   "metadata": {},
   "source": [
    "If you get 1.0 as an output that means that the person potentially has diabetes, bad for the person but good for us on catching that!\n",
    "\n",
    "Finally, run the next command to see where all this machine learning thing is going to... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8026c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/rand/r_everywhere.jpg\", width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3b967",
   "metadata": {},
   "source": [
    "# Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452e2f1",
   "metadata": {},
   "source": [
    "In this tutorial, we have covered the following topics when creating a deep neural network for a simple classification task using Keras:\n",
    "\n",
    "- Load and split data.\n",
    "- Define the model.\n",
    "- Compile the model.\n",
    "- Fit the model.\n",
    "- Evaluate the model.\n",
    "- Predict with the model.\n",
    "\n",
    "I hope that you have enjoyed it, and I invite you to keep exploring this awesome topic. Here is a link with multiple dataset repositories for you to play with: https://archive.ics.uci.edu/ml/index.php. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856d8ac",
   "metadata": {},
   "source": [
    "# Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152fc19",
   "metadata": {},
   "source": [
    "This  example has been modified and extended from the following blog:\n",
    "\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "Many thanks to the author!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
