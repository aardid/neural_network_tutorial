{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246a718c",
   "metadata": {},
   "source": [
    "# Coding a simple deep neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac38e5a",
   "metadata": {},
   "source": [
    "Similar to the previous tutorial we will try to predict a single binary output given the three binary inputs. Here we will implement a 3 layer neural network. One layer for the input, a second hidden layer, and a final layer for the output. The field of adding more layers to model more combinations of relationships such as this is known as \"deep learning\" because of the increasingly deep layers being modeled.\n",
    "\n",
    "Our new dataset is the following:\n",
    "\n",
    "                                 Input\t     Output\n",
    "                                0, 0, 1\t       0\n",
    "                                0, 1, 1\t       1\n",
    "                                1, 0, 1\t       1\n",
    "                                1, 1, 1\t       0\n",
    "\n",
    "Why using a more complex model? In the previous example the first input column was directly correlated with the output (did you noticed it?). So, a simple linear model to predict the output as a combination of the input and weights was enough. However, in this new dataset input columns and output are not linearly correlated. In consequence, we need to add another layer of complexity that will introduce nonlinearity when calculating the output as a combination of the inputs. We will introduce a second hidden layer of 4 neurons.\n",
    "\n",
    "Some tips: \n",
    "- You will need to create two weigth vectors, 'w0' from layer 0 to layer 1, and 'w1' from layer 1 to layer 2 (randomly initialize with mean 0 both). Be careful with the shapes, they need to be consistent with the new layer structure of 3-4-1 neurons. \n",
    "- Update the weigth from the second hidden layer to the third one should be trivial, just follow what was done in th eprevious tutorial. The tricky part is on how to update the weigths that connect the first two layers. We need to calculate the error in the hidden layer 1 and then use it to calculate a delta and update the weights. We do this by backpropagating the 'error weighted derivative' from the last layer using the weigths, i.e. by calculating the dot product of both. Again, be careful with the shapes. \n",
    "\n",
    "To do:\n",
    "- Complete the code given below to train a deep neural network coherent with the dataset and a hiden layer with 4 neurons. Check that the test is passed after completion.\n",
    "\n",
    "Note: From now on, you will need to fill the _ (underscores) in the code given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as _\n",
    "\n",
    "# sigmoid function and its derivative\n",
    "def nonlin(x):\n",
    "    return _\n",
    "# derivative of sigmoid function\n",
    "def dnonlin(x):\n",
    "    return _\n",
    "\n",
    "# input dataset\n",
    "X = np.array([[_,_,_],\n",
    "                [_,_,_],\n",
    "                [_,_,_],\n",
    "                [_,_,_]])\n",
    "\n",
    "# input dataset\n",
    "y = np.array([[_],\n",
    "              [_],\n",
    "              [_],\n",
    "              [_]])\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "w0 = 2*np.random.random((_,_)) -1 # (3,4) : 3 columns, 4 rows \n",
    "w1 = 2*np.random.random((_,_)) -1 \n",
    "\n",
    "# iterate for performing backpropagation \n",
    "for ite in range(_):\n",
    "    # (1) Forward propagation to calculate layer values\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(_,_))\n",
    "    l2 = nonlin(np.dot(_,_))\n",
    "    # (2) Calculate error with current l2\n",
    "    e2 = y - _ \n",
    "    # (3) Calculate delta l2: current error by the slope of \n",
    "    # the sigmoid at the values in l2\n",
    "    d2 = _ * dnonlin(_)\n",
    "    # (4) Update weigths 1 (l1 - > l2)\n",
    "    w1 = w1 + np.dot(_.T,_)\n",
    "    \n",
    "    # (5) Calculate error in l1 (backpropagated the error from l2) \n",
    "    e1 = np.dot(_, _.T)\n",
    "    # (6) Calculate delta l1\n",
    "    d1 = _ * dnonlin(_)\n",
    "    # (7) Update weigths 0 (l0 -> l1)\n",
    "    w0 = w0 + np.dot(_.T,_)\n",
    "\n",
    "# check results\n",
    "for i, yt in enumerate(y):\n",
    "    if yt != int(np.round(l2[i])):\n",
    "        print('Test not passed')\n",
    "        print('Output:')\n",
    "        print(np.round(l2))\n",
    "        break\n",
    "    else:\n",
    "        if i == len(y)-1:\n",
    "            print('Test passed!')\n",
    "            print('Layer 2 after last iteration:')\n",
    "            print(np.round(l2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61055d46",
   "metadata": {},
   "source": [
    "# Final notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91fb01",
   "metadata": {},
   "source": [
    "Congrats, you just finish this awesome tutorial. \n",
    "\n",
    "To do:\n",
    "- Run the following comand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfaffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/rand/SuccessKid.jpg\", width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10762cb",
   "metadata": {},
   "source": [
    "Up to a bigger challenge?\n",
    "Try to develope a code that replicates the neural network define in the first tutorial usign keras and train it with the diabetes database. Test it using the same train and test batches. How is the accuracy obtained by your network compared to the keras one? \n",
    "\n",
    "When developing and comparing machine learning techniques, many undesaribles mistakes can be made. For a simple review on what to avoid when working in machine learning check the following paper: https://arxiv.org/abs/2108.02497\n",
    "\n",
    "Machine learning is a huge field, and we have just scratched the surface. We encourage you to keep learning and develop new ideas on how this techniques can be implemented in yout fild. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
